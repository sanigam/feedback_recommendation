{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "#Ref: Code from https://medium.com/analytics-vidhya/introduction-to-bert-f9aa4075cf4f used here\n",
    "#Install following libraries before first run. For subsequent runs, you may comment these\n",
    "# !pip install tensorflow_hub\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece\n",
    "#!pip install tensorflow-gpu\n",
    "\n",
    "#Importing Required libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "from bert import bert_tokenization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#Tensorflow version\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "df = pd.read_csv('./data_with_mapped_phrases.csv') \n",
    "#This has this will have 5 columns 1) annotation, 2) student_note , 3) teacher_feedback 4) phrases  & 5) score , \n",
    "#score column is for reference and it is not used for BERT or T5 trainining\n",
    "# teacher_feedback column is used for T5 model traiining while phrases column is used for BERT model\n",
    "#print ( f'Data Shape: {df.shape} ') #Number of rows and column in data-frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (66317, 9) \n",
      "Data Shape: (59609, 9) \n",
      "Data Shape: (59609, 9) \n",
      "Data Shape: (59602, 9) \n",
      "Data Shape: (50981, 9) \n",
      "Data Shape: (50689, 9) \n",
      "Data Shape: (50406, 9) \n",
      "Data Shape: (50406, 9) \n"
     ]
    }
   ],
   "source": [
    "#Data cleaning\n",
    "df = df[df['student_note'].notnull()]\n",
    "print ( f'Data Shape: {df.shape} ') \n",
    "df = df[df['annotation'].notnull()]\n",
    "print ( f'Data Shape: {df.shape} ') \n",
    "df = df[df['teacher_feedback'].notnull()]\n",
    "print ( f'Data Shape: {df.shape} ') \n",
    "df = df[df.student_note != 'note' ]\n",
    "print ( f'Data Shape: {df.shape} ')\n",
    "\n",
    "df['annotation'] = df['annotation'].str.strip()\n",
    "df['student_note'] = df['student_note'].str.strip()\n",
    "\n",
    "df = df [df['annotation'].str.len() > 3]\n",
    "print ( f'Data Shape: {df.shape} ')\n",
    "df = df [df['student_note'].str.len() > 3]\n",
    "print ( f'Data Shape: {df.shape} ')\n",
    "\n",
    "df = df.drop_duplicates(subset=['annotation', 'student_note'])\n",
    "print ( f'Data Shape: {df.shape} ') \n",
    "print ( f'Data Shape: {df.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking into data, cleared output so as not to expose data\n",
    "display(df.sample(2)) #Sample rows of dataframe\n",
    "\n",
    "df['phrases'].value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (40324, 9)  ,  Test Data Shape:  (10082, 9)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into test and training. This is used for training/testing BERT as well as T5 model\n",
    "df_train, df_test = train_test_split( df , test_size=0.2, random_state=42)\n",
    "\n",
    "print( f'Training Data Shape: {df_train.shape}  ,  Test Data Shape:  {df_test.shape}') # Rows/Cols in train/test data\n",
    "\n",
    "df_train.to_csv ('train_data.csv' , index=False)\n",
    "df_test.to_csv ('test_data.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of vocab in our tokenizer :  30522\n"
     ]
    }
   ],
   "source": [
    "#Loading BERT Standard model (Pretrained Model on Wikipedia and Book Corpus)\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True, name = 'keras_bert_layer' )\n",
    "\n",
    "#Getting vocab file from bert layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() \n",
    "\n",
    "do_lower_case = True  # For uncased model it will True \n",
    "\n",
    "#Defining tokenizer object which will be used to tokenize text before feeding to bert\n",
    "tokenizer_for_bert = bert_tokenization.FullTokenizer(vocab_file, do_lower_case) #Tokenizer to tokenize input text\n",
    "\n",
    "print ( '\\nLength of vocab in our tokenizer : ' , len(tokenizer_for_bert.vocab) ) #BERT vocab has around 30K words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text after encoding:  (array([[  101,  5754, 17287,  3508,  1024,  6160,  2000,  2691, 15909,\n",
      "         2622,   102,     0]]), array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "#Function to encode text in format to feed to BERT\n",
    "\n",
    "def encode_text_for_bert2 (texts, tokenizer_for_bert, max_len=512):\n",
    "    ''' This function is to encode data for inputting into BERT model\n",
    "    Parameters:\n",
    "    texts - List of texts to encode\n",
    "    tokenizer_for_bert - Tokenizer to be used to convert text into tokens\n",
    "    max_len - Maximum length of text. It can have maximum value as 512\n",
    "    Return: Tupple of 3 numpy arrays \n",
    "    1) Token Ids padded with 0s to make length as max_len.  \n",
    "    2) Array where we have 1 for actual tokens and 0 for padding tokens\n",
    "    3) Array of 0s to indicate that token belongs to 1st sentence (chunk of text). There is no 2nd sentence here.\n",
    "    '''\n",
    "    all_token_ids = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        [text1, text2] = text.split(' Student Note: ')[:2]\n",
    "        text1 = text1.strip()\n",
    "        text2 = text2.strip()\n",
    "        \n",
    "        tokens = tokenizer_for_bert.tokenize(text1) + [\"[SEP]\"] +  tokenizer_for_bert.tokenize(text2)\n",
    "            \n",
    "        tokens = tokens[:max_len-3] # Truncating number of tokens to max_len -3, Reduced extra 3 to add special tokens\n",
    "        \n",
    "        input_sequence = [\"[CLS]\"] + tokens + [\"[SEP]\"]  # [CLS] and [SEP] are special tokens to be added into input text\n",
    "        \n",
    "        pad_len = max_len - len(input_sequence) # Spaces to fill with 0s to make each sequence equal to max_len\n",
    "        \n",
    "        token_ids = tokenizer_for_bert.convert_tokens_to_ids(input_sequence)   #Converting tokens to token ids \n",
    "       \n",
    "        token_ids += [0] * pad_len  #Padding token ids with 0s\n",
    "        \n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len # 1 where we have sentence tokens and 0 otherwise\n",
    "        \n",
    "        segment_ids = [0] * max_len # Segment ids are all 0 to indicate it is part of sentence 1. There is no sentence 2 here\n",
    "        \n",
    "        all_token_ids.append(token_ids)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_token_ids), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "#Optional Step: This is just to understand input/output of function encode_text_for_bert\n",
    "test_text =  \"Annotation: Welcome to  CommonLit project, Student Note: Yes I agree \"\n",
    "\n",
    "print (\"Test text after encoding: \" ,encode_text_for_bert2 ( [test_text], tokenizer_for_bert, 12) ) # Pl Note id 101 is for token [CLS] and 102 for token [SEP]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating BERT  Model \n",
    "def bert_model_creation (bert_layer, max_len=512, model_type = 'Classification', num_classes = 2):\n",
    "    '''This function is to create BERT model for Classification or Regession Task\n",
    "    Parameters:\n",
    "    model_type = 'Classification' for classification task or 'Regression' for regression task. \n",
    "    num_classes = Number of classes in classification task. Value of 2 means binary classification. More than 2 for multiclass classification.\n",
    "                  For regression, num_classes parameter is ignored.\n",
    "    Return: Deep Learning Model\n",
    "    Important: You may add additional dense layers in place holder provided as \"***PLACEHOLDFER FOR ADDITIONAL LAYERS****\"\n",
    "    '''   \n",
    "    #Input to bert layer\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    #Output from bert layer\n",
    "    bert_layer_out = bert_layer([input_word_ids, input_mask, segment_ids]) # Python list of 2 tensors with shape (batch_size, 768) and (batch_size, max_len, 768)\n",
    "    \n",
    "    #Extrating Embedding for CLS token comming out of bert layer. Note CLS is the first token\n",
    "    cls_out = bert_layer_out[1][:,0,:] # Getting hidden-state of 1st tokens from second tensor in bert_layer_out, Tensor shape - (batch size, 768) \n",
    "    cls_out, _ = bert_layer_out  #Taking pooled output\n",
    "    \n",
    "    \n",
    "    #***PLACEHOLDFER FOR ADDITIONAL LAYERS****. \n",
    "    #Add more layers here if you want. See example below\n",
    "    cls_out = Dropout(.50) (cls_out)\n",
    "    cls_out = Dense(500, activation='relu')(cls_out) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Defines last layer depending on model type and  number of classes. Activation function is used depending on model_type and num_classes\n",
    "    if model_type == 'Classification' :\n",
    "        if num_classes == 2 :\n",
    "            out = Dense(1, activation='sigmoid')(cls_out)     # ** For Binary classification, use sigmoid activation\n",
    "        else:    \n",
    "            out = Dense(num_classes, activation='softmax')(cls_out) # For Multi Class classification, use softmax activation\n",
    "    else:\n",
    "        out = Dense(1, activation='linear')(cls_out)     # For regression, use linear activation\n",
    "    \n",
    "    #Model creation using inputs and output\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out, name='deeplearning_bert__model')\n",
    "    \n",
    "    \n",
    "    \n",
    "    learning_rate = 2e-6 # modify learning rate,as needed\n",
    "    \n",
    "    #Compiles Model depending on model type and number of classes. Loss function as well as metrics is used accordingly\n",
    "    if model_type == 'Classification' :\n",
    "        if num_classes == 2 :\n",
    "            model.compile(Adam(learning_rate= learning_rate), loss='binary_crossentropy', metrics=['acc']) # ** For Binary classification\n",
    "        else:\n",
    "            model.compile(Adam(learning_rate= learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy']) # For Multi Class classification \n",
    "    else:\n",
    "        model.compile(Adam(learning_rate= learning_rate), loss='mse', metrics=['mse']) # For Regression\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256 #Max length of text input to model. It can go up to 512. Keeping it small to run it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deeplearning_bert__model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_bert_layer (KerasLayer)  [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
      "                                 (None, 256, 768)]                'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 768)          0           ['keras_bert_layer[0][0]']       \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 500)          384500      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 21)           10521       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,877,262\n",
      "Trainable params: 109,877,261\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building Classification Model\n",
    "#modify values of model_type and num_classes as per need\n",
    "model = bert_model_creation(bert_layer, max_len=max_len, model_type = 'Classification', num_classes = 21) #binary classification as num_classes = 2\n",
    "\n",
    "#Model Summary. Pl note, there are ~109 Million parameters as it is BERT standard model\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Training Data for BERT.  If you want  preprocessing/cleaning of input text, it should be done before this step\n",
    "train_input = encode_text_for_bert2((df_train['annotation']+ ' Student Note: ' + df_train['student_note']).values, tokenizer_for_bert, max_len= max_len)\n",
    "\n",
    "#Output variable  for multi-class classification \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = df_train['phrases'].values \n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "filehandler = open(\"le.obj\",\"wb\")\n",
    "pickle.dump(le,filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional step: Checking accuracy on train data before fine-tuning so that we can see improvement by fine tuning\n",
    "# predictions = model.predict(train_input)\n",
    "# accuracy_score (y_train,  np.argmax(predictions, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f83582f3560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f83582f3560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f83582f3560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1135/1135 [==============================] - ETA: 0s - loss: 2.6518 - sparse_categorical_accuracy: 0.2309WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f84283baa70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f84283baa70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f84283baa70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1135/1135 [==============================] - 1093s 948ms/step - loss: 2.6518 - sparse_categorical_accuracy: 0.2309 - val_loss: 2.4947 - val_sparse_categorical_accuracy: 0.2680\n",
      "Epoch 2/5\n",
      "1135/1135 [==============================] - 1074s 947ms/step - loss: 2.4978 - sparse_categorical_accuracy: 0.2566 - val_loss: 2.4342 - val_sparse_categorical_accuracy: 0.2700\n",
      "Epoch 3/5\n",
      "1135/1135 [==============================] - 1073s 946ms/step - loss: 2.4305 - sparse_categorical_accuracy: 0.2661 - val_loss: 2.3903 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 4/5\n",
      "1135/1135 [==============================] - 1075s 947ms/step - loss: 2.3590 - sparse_categorical_accuracy: 0.2820 - val_loss: 2.3587 - val_sparse_categorical_accuracy: 0.2859\n",
      "Epoch 5/5\n",
      "1135/1135 [==============================] - 1075s 947ms/step - loss: 2.2699 - sparse_categorical_accuracy: 0.3093 - val_loss: 2.3500 - val_sparse_categorical_accuracy: 0.2842\n"
     ]
    }
   ],
   "source": [
    "#Model Training (Fine-tuning for  classification) \n",
    "epochs = 5    #Modify as neded\n",
    "batch_size = 32  #Modify as needed\n",
    "train_history = model.fit(train_input, y_train ,  validation_split=0.10 , epochs= epochs,batch_size= batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5cElEQVR4nO3deXhV1dX48e/KzBCmBMIQIvM8E0CcEAcGBxza4og4Ilpb7fs61L7Vakd/2lq1VZEqRcVKkYLSihJwQC0gCYhAwjwmDCEECATIvH5/nANcwgUukJOTYX2eJ0/uPXufe9c9hLPu2fvsvUVVMcYYY8oL8zsAY4wxVZMlCGOMMUFZgjDGGBOUJQhjjDFBWYIwxhgTlCUIY4wxQVmCMAYQkcki8tsQ624WkSu8jskYv1mCMMYYE5QlCGNqEBGJ8DsGU3NYgjDVhtu085iILBeRgyLylogkiMgnInJAROaJSOOA+qNEJF1E9onIlyLSNaCsr4gsdff7JxBT7r2uEZFl7r4LRKRXiDFeLSLfich+EckUkWfKlV/kvt4+t/xOd3sdEfmTiGwRkTwR+cbddqmIZAU5Dle4j58RkekiMkVE9gN3ishAEVnovscOEfmriEQF7N9dROaKyB4RyRaRX4hIcxE5JCJxAfX6i0iOiESG8tlNzWMJwlQ3PwCuBDoB1wKfAL8A4nH+nn8KICKdgPeBR4CmwGzg3yIS5Z4sPwTeBZoAH7ivi7tvP2AScD8QB7wBzBKR6BDiOwjcATQCrgYeEJHr3ddNcuP9ixtTH2CZu98fgf7ABW5MjwNlIR6T64Dp7nu+B5QCP8M5JoOBy4EH3RhigXnAp0BLoAPwmaruBL4ERge87u3AVFUtDjEOU8NYgjDVzV9UNVtVtwFfA9+q6neqWgjMBPq69W4CPlbVue4J7o9AHZwT8PlAJPCSqhar6nQgNeA97gPeUNVvVbVUVd8GCt39TklVv1TVFapapqrLcZLUELf4NmCeqr7vvm+uqi4TkTDgbuBhVd3mvucC9zOFYqGqfui+52FVXaKqi1S1RFU34yS4IzFcA+xU1T+paoGqHlDVb92yt3GSAiISDtyCk0RNLWUJwlQ32QGPDwd5Xt993BLYcqRAVcuATKCVW7ZNj5+pckvA4/OA/3WbaPaJyD6gtbvfKYnIIBH5wm2ayQPG43yTx32NDUF2i8dp4gpWForMcjF0EpH/iMhOt9np9yHEAPAR0E1E2uFcpeWp6uKzjMnUAJYgTE21HedED4CICM7JcRuwA2jlbjsiKeBxJvA7VW0U8FNXVd8P4X3/AcwCWqtqQ2ACcOR9MoH2QfbZDRScpOwgUDfgc4TjNE8FKj8l8+vAaqCjqjbAaYI7XQyoagEwDedKZwx29VDrWYIwNdU04GoRudztZP1fnGaiBcBCoAT4qYhEiMiNwMCAff8GjHevBkRE6rmdz7EhvG8ssEdVC0RkIHBrQNl7wBUiMtp93zgR6eNe3UwCXhSRliISLiKD3T6PtUCM+/6RwC+B0/WFxAL7gXwR6QI8EFD2H6C5iDwiItEiEisigwLK3wHuBEYBU0L4vKYGswRhaiRVXYPTnv4XnG/o1wLXqmqRqhYBN+KcCPfi9FfMCNg3Dacf4q9u+Xq3bigeBH4tIgeAp3ES1ZHX3QpchZOs9uB0UPd2ix8FVuD0hewB/h8Qpqp57mu+iXP1cxA47q6mIB7FSUwHcJLdPwNiOIDTfHQtsBNYBwwNKP8vTuf4Urf/wtRiYgsGGWMCicjnwD9U9U2/YzH+sgRhjDlKRAYAc3H6UA74HY/xlzUxGWMAEJG3ccZIPGLJwYBdQRhjjDkJu4IwxhgTVI2a2Cs+Pl7btGnjdxjGGFNtLFmyZLeqlh9bA9SwBNGmTRvS0tL8DsMYY6oNEdlysjJrYjLGGBOUJQhjjDFBWYIwxhgTlKd9ECIyAngZCAfeVNXnypVfB/wGZ2h/Cc7919+4ZZNwpibepao9zjaG4uJisrKyKCgoONuXqBZiYmJITEwkMtLWdjHGVAzPEoQ76+SrOPO+ZAGpIjJLVTMCqn0GzFJVdVfsmgZ0ccsm48yF8865xJGVlUVsbCxt2rTh+Mk7aw5VJTc3l6ysLNq2bet3OMaYGsLLJqaBwHpV3ehOjjYVZ+Wro1Q1P2BO/noETFusql/hTFp2TgoKCoiLi6uxyQFARIiLi6vxV0nGmMrlZYJoxfELmWS5244jIjeIyGrgY5xVtc6IiIwTkTQRScvJyTlZnTN92WqnNnxGY0zl8jJBBDtjnTCvh6rOVNUuwPU4/RFnRFUnqmqyqiY3bRp0rIcxxtRYS7fuZeJXZ7sY4al5mSCycFbwOiIRZ5WvoNwmpfYiEn+yOtXRvn37eO211854v6uuuop9+/ZVfEDGmBrji9W7uPVvi/jHt1s5WFhS4a/vZYJIBTqKSFsRiQJuxlmK8SgR6XBk2UcR6QdEAbkexlTpTpYgSktLT7nf7NmzadSokUdRGWOqu+lLsrj3nTQ6NKvPB+MvoF50xd9z5NldTKpaIiIPAXNwbnOdpKrpIjLeLZ8A/AC4Q0SKcRacv+lIp7WIvA9cCsSLSBbwK1V9y6t4vfLzn/+cDRs20KdPHyIjI6lfvz4tWrRg2bJlZGRkcP3115OZmUlBQQEPP/ww48aNA45NG5Kfn8/IkSO56KKLWLBgAa1ateKjjz6iTp06Pn8yY4wfVJU3vtrIc5+s5qIO8UwY05/6HiQHqGHTfScnJ2v5uZhWrVpF165dAXj23+lkbN9foe/ZrWUDfnVt95OWb968mWuuuYaVK1fy5ZdfcvXVV7Ny5cqjt6Pu2bOHJk2acPjwYQYMGMD8+fOJi4s7LkF06NCBtLQ0+vTpw+jRoxk1ahS33377Ce8V+FmNMTVPWZny249XMem/m7i2d0v+9KPeREWcW0OQiCxR1eRgZTVqsr7qYODAgceNVXjllVeYOXMmAJmZmaxbt464uLjj9mnbti19+vQBoH///mzevLmywjXGVBFFJWU8+sH3zPp+O3dd2Ianru5GWJi3dy/WqgRxqm/6laVevXpHH3/55ZfMmzePhQsXUrduXS699NKgYxmio6OPPg4PD+fw4cOVEqsxpmrILyzhgSlL+Hrdbp4Y0YXxQ9pVyq3ttSpB+CE2NpYDB4Kv3piXl0fjxo2pW7cuq1evZtGiRZUcnTGmqtudX8hdf08lY8d+XvhhL36U3Pr0O1UQSxAei4uL48ILL6RHjx7UqVOHhISEo2UjRoxgwoQJ9OrVi86dO3P++ef7GKkxpqrZmnuIOyZ9y879Bfztjv5c1iXh9DtVoFrVSV3T1abPakxNl749j7GTUikpK+OtsQPof15jT97HOqmNMaYaWbBhN+PeWUKDmAimjhtMh2axvsRhCcIYY6qQ2St28MjUZZwXV5d37hlIi4b+jXmyBGGMMVXEuws38/SsdPonNebNsck0qhvlazyWIIwxxmeqyp/nruWVz9dzRddm/OWWftSJCvc7LEsQxhjjp5LSMp76aCXvL87kpuTW/O6GHkSEV43VoC1BGGOMTwqKS/nJ+98xNyObh4Z24H+HdapSa7tUjTRVg53tdN8AL730EocOHargiIwxVUHeoWLGvPUt81Zl8+yo7jw6vHOVSg5gCcJzliCMMeXtzCtg9BsL+T4zj7/c0pexF7TxO6SgrInJY4HTfV955ZU0a9aMadOmUVhYyA033MCzzz7LwYMHGT16NFlZWZSWlvLUU0+RnZ3N9u3bGTp0KPHx8XzxxRd+fxRjTAVYvyufsZMWk3e4mMl3DeCCDlV3jbTalSA++TnsXFGxr9m8J4x87qTFzz33HCtXrmTZsmWkpKQwffp0Fi9ejKoyatQovvrqK3JycmjZsiUff/wx4MzR1LBhQ1588UW++OIL4uOr7h+QMSZ0S7fu5e7JqUSEhTF13Pn0aNXQ75BOyZqYKlFKSgopKSn07duXfv36sXr1atatW0fPnj2ZN28eTzzxBF9//TUNG1btPxpjzJk7sjxowzqR/OuBwVU+OUBtu4I4xTf9yqCqPPnkk9x///0nlC1ZsoTZs2fz5JNPMmzYMJ5++mkfIjTGeOFfS7J4/F/L6doilr/fOZCmsdGn36kK8PQKQkRGiMgaEVkvIj8PUn6diCwXkWUikiYiF4W6b3URON338OHDmTRpEvn5+QBs27aNXbt2sX37durWrcvtt9/Oo48+ytKlS0/Y1xhT/agqb8zfwP9+8D3nt2vC1HGDq01yAA+vIEQkHHgVuBLIAlJFZJaqZgRU+wyYpaoqIr2AaUCXEPetFgKn+x45ciS33norgwcPBqB+/fpMmTKF9evX89hjjxEWFkZkZCSvv/46AOPGjWPkyJG0aNHCOqmNqWbKypTfzV7FW99s4ppeLfjT6N5ER/g/OvpMeDbdt4gMBp5R1eHu8ycBVPUPp6g/SVW7num+R9h037XnsxpTlRWVlPHY9O/5aNl27rygDU9f4/3yoGfLr+m+WwGZAc+zgEHlK4nIDcAfgGbA1Weyr7v/OGAcQFJS0jkHbYwx5yJwedDHR3TmgSHtq9wAuFB52QcR7IiccLmiqjNVtQtwPfCbM9nX3X+iqiaranLTpk3PNlZjjDlnu/MLufVvi1iwIZfnf9iLBy/tUG2TA3h7BZEFBC6emghsP1llVf1KRNqLSPyZ7ns6qlqt/5FCUZNWBjSmOsrcc4gxbznLg04c05/Lu1bu8qBe8PIKIhXoKCJtRSQKuBmYFVhBRDqIe+YWkX5AFJAbyr6hiomJITc3t0afQFWV3NxcYmJi/A7FmFopfXseN76+gL2Hinnv3kE1IjmAh1cQqloiIg8Bc4BwnA7odBEZ75ZPAH4A3CEixcBh4CZ1zuRB9z2bOBITE8nKyiInJ6cCPlXVFRMTQ2Jiot9hGFPrLNyQy7h30qgfE8E/xg+mY4I/y4N6wbO7mPwQ7C4mY4zxSuDyoG/fPZCWjfxbHvRs+XUXkzHG1FjvLtrC0x+tpF9SY96qAsuDesEShDHGnAFV5c/z1vHKZ+u4vEsz/npr1Vge1AuWIIwxJkTO8qDpvL94K6OTE/n9DT2rzPKgXrAEYYwxISgoLuWn739HSkY2Px7ankeHVb0V4CqaJQhjjDmNvMPF3Pd2Gqlb9vDMtd2488K2fodUKSxBGGPMKezMK2DspMVs3J3PX27pyzW9WvodUqWxBGGMMSdxZHnQfYeKmHzXQC6swsuDesEShDHGBPGduzxoeJjwz/urxwpwFc0ShDHGlPPFml08OGUpTWOjefeegZwXV8/vkHxhCcIYYwIcWR60S/NYJt9VfZYH9YIlCGOMcU38agO/n72aCzvEMeH2/sTGRPodkq8sQRhjar2yMuX3s1fx5jebuLpXC16shsuDesEShDGmVisqKePx6d/zYTVYHrSyWYIwxtRaBwtLGO8uD/rY8M48eGn1XR7UC5YgjDG1Um5+IXdPTmXl9v08/4NejB7Q+vQ71TKWIIwxtU7mnkPcMWkx2/cd5o3b+3NFt5qxAlxF83QaQhEZISJrRGS9iPw8SPltIrLc/VkgIr0Dyh4WkZUiki4ij3gZpzGm9sjYvp8bX1/AnoNFvHfvIEsOp+BZghCRcOBVYCTQDbhFRLqVq7YJGKKqvYDfABPdfXsA9wEDgd7ANSLS0atYjTG1w8INudz0xkIiwoQPxg8muU0Tv0Oq0ry8ghgIrFfVjapaBEwFrgusoKoLVHWv+3QRcGRR5a7AIlU9pKolwHzgBg9jNcbUcJ+s2MHYSYtJaBjDvx64gE41aO1or3iZIFoBmQHPs9xtJ3MP8In7eCVwiYjEiUhd4CogaA+SiIwTkTQRScvJyamAsI0xNc2URVt48B9L6dGqAdPHD66Wa0f7wctO6mD3imnQiiJDcRLERQCqukpE/h8wF8gHvgdKgu2rqhNxm6aSk5ODvr4xpnZSVV6at46Xa8HyoF7w8goii+O/9ScC28tXEpFewJvAdaqae2S7qr6lqv1U9RJgD7DOw1iNMTVMaZnyi5krefmzdfyofyJvjOlvyeEMeXkFkQp0FJG2wDbgZuDWwAoikgTMAMao6tpyZc1UdZdb50ZgsIexGmNqkILiUh6e+h1z0rN58NL2PDa85i8P6gXPEoSqlojIQ8AcIByYpKrpIjLeLZ8APA3EAa+5/3glqprsvsS/RCQOKAZ+HNCZbYwxJxW4POivru3GXbVkeVAviGrNabZPTk7WtLQ0v8Mwxvgke7+zPOiGnHz+NLoPo3rXnuVBz5aILAn4Yn4cG0ltjKkRNuTkc8dbzvKgf79zIBd1rF3Lg3rBEoQxptoLXB506rjB9EysfcuDesEShDGmWgtcHvSduwfSJr52Lg/qBUsQxpiqo/gwrJsL+7ZCTAOIjoXoBhDT0PkdHetsj6wLIsxYmsXj05fTKSGWyXcPoFlsjN+foEaxBGGM8VdxAayfB+kzYc0nUHzw9PtIOAXh9RhQFM28erEk1k8g4t9uEjkusTSA6IbHEktgkoluAGE2LuJULEEYYypfSSFs+NxJCqtnQ9EBqNMEev0Iut8ALXpD0UEo2A+F+4/9LtxP2eH9LMzYyLrMHXRtrCS3iCC8KB/2b4fC1cfqlgWdfOF4UfXLJZPAxw2Of3zCFY37OLLmXrVYgjDGVI6SItj4JaTPgNUfOyfxmEbQ/XonKbS9BMIjj9Wv0xjK9TUXl5bx+PTlzNzUkbGDz+OOa7sHXx5UFUoK3GRxAArzApLMgXKP844loYL9kJd1rLz40Ok/V3jUiVcmJ00sQcqjY51EFebp6gtnxRKEMcY7pcWwcb57pfBv52Qc3RC6XusmhSEQERXSSx0sLOGB95by1dqc0y8PKgKRdZyf2HNY76G05OiVy3GJJeCK5lgSCti+b4v7OM8p07LTvJGcmGRCSSxH6zaEuhU/dbklCGNMxSotgc1fOUlh1b/h8F7nJNb5KuhxI7S7FCKiz+gljywPumJbHv/vBz25aUCSN7GXFx7hnHjP5eSr6jSXHZdMQriiyd8FuRuO7VdaePL3qBsHj288+xhPwhKEMebclZbAlv86zUer/g2Hcp1mk85XOVcK7S8767b645YHHZPMldVtBTgRiK7v/DQ4h5HdJYXHJ5DAxOLRjBiWIIwxZ6esFLYuhJUzYNUsOJgDkfWg8wgnKXS4wmniOQerduxn7KTFFBSX8t69g2r3CnAR0c5PvcobIW4JwhgTurIyyFzkNB9lfAT52RBRBzoNd5qPOlwJUXUr5K0WbczlvrfTqBcdwXRbAc4XliCMMadWVgZZqW5S+BAO7ICIGOh4JXS/0UkOURU7evnTlTv46dRlJDWpy9t3D6SVrQDnC0sQxpgTqcK2JU5SSP8Q9mdBeLSbFG5wkkJ0xX+jLygu5a1vNvHHlDX0ad2ISWMH0LheaHc5mYpnCcIY41CF7d85Hc3pH0HeVgiLdPoSLn8aOo90bqv0QHFpGdPSMnnls3Vk7y9kZI/mvDi6j60A5zNLEMbUZqqw43v3SmGmc/9+WIRz19HQJ527kOo08uzty8qUfy/fzp/nrmVz7iH6n9eYV27uy6B2cZ69pwmdpwlCREYAL+OsKPemqj5Xrvw24An3aT7wgKp+75b9DLgXUGAFcJeqFngZrzG1gipkrzyWFPZsBAl3xicMedxJCh4Mujo+BOWLNbt4Yc5aVu3YT5fmsbw1NpnLujSzpUGrEM8ShIiEA68CVwJZQKqIzFLVjIBqm4AhqrpXREYCE4FBItIK+CnQTVUPi8g0nDWtJ3sVrzE1mirsWuU2H82E3PVOUmh7CVz4iDOy2eOkcMTiTXt4/tPVpG3Zy3lxdXn55j5c26tl8CkzjK+8vIIYCKxX1Y0AIjIVuA44miBUdUFA/UVAYrnY6ohIMVAX2O5hrMbUTLtWH7tS2L0GJAzaXASDfwxdR1XqPfUrt+Xxwpw1zF+bQ0KDaH53Qw9GJ7cmMrzqzUFkHF4miFZAZsDzLGDQKerfA3wCoKrbROSPwFbgMJCiqinBdhKRccA4gKSkShp+b0xVtnvdsaSwKwMQOO9CGHgfdLsO6jer1HA25uTz4ty1/Gf5DhrVjeTJkV0Ye0EbYiKtA7qq8zJBBLteDDoeXESG4iSIi9znjXGuNtoC+4APROR2VZ1ywguqTsRpmiI5Odmb8ebGVHW5G9zmow+d/gWApMEw8gXoNgpim1d6SDvyDvPyvHV8sCSL6IgwfnJZB+67pB0NYiJPv7OpErxMEFlA64DniQRpJhKRXsCbwEhVzXU3XwFsUtUct84M4ALghARhTK21Z9OxK4Wdy51trQfBiOecK4VzmffnXMI6WMRrX6znnUVbQGHM+efx0GUdiK9/ZhP0Gf95mSBSgY4i0hbYhtPJfGtgBRFJAmYAY1R1bUDRVuB8EamL08R0OZDmYazGVA97tzijmdNnOmMWAFolw7DfOUmhUetT7u6l/MIS3vx6I29+vYlDRSX8oF8iD1/RkcTGFTP1hql8niUIVS0RkYeAOTi3uU5S1XQRGe+WTwCeBuKA19xb20pUNVlVvxWR6cBSoAT4DrcZyZhaJy/LaTpKnwnb3O9JLfvClb+GbtdD4/P8jI6C4lKmLNrCa19uYM/BIkb2aM7/DutEh2Y2d1J1JxrCNLEi8i9gEvCJ6mlXvvBNcnKypqXZhYapAfZvdybDWzkDshY725r3cibE63Y9NGnra3gAJaVlTF+SxcufrWNHXgEXd4znseGd6ZXYyO/QzBkQkSWqmhysLNQriNeBu4BXROQDYLKqrq6oAI0xwIGdTlJIn+lMow2Q0BMue8qZ/yiuvb/xucrKlNkrd/Biylo27j5In9aN+NPo3lzQvvJumTWVI6QEoarzgHki0hC4BZgrIpnA34ApqlrsYYzG1Fz5u9yk8KGz4A4KzbrB0P9zrhSadvI5wGNUlS/X5vDHOWtI376fzgmx/O2OZK7oaqOfa6qQ+yBEJA64HRiD0yfwHs5tqWOBS70Izpga6eBuZ4GdlTOcpKBlEN8JhjzhXCk06+J3hCdI27yH5z9dw+LNe2jdpA5/vqk3o3q3ItxGP9doISUI9zbTLsC7wLWqusMt+qeIWKO/MadzaI+TFNJnwqavQUshrgNc/KibFLo6S1NWMRnb9/PHlDV8vnoXTWOj+c113blpQBJRETb6uTYI9Qrir6r6ebCCk3VuGFNrlRQ68x7tXOGMT9ix3FlwR0uhcVu46BEnKST0qJJJAWDz7oO8OHcts77fToOYCJ4Y0YWxF5xH3SibALo2CfVfu6uILFXVfXB0pPMtqvqaZ5EZUx0c3gs7Vx5LBjtXQM5qKCtxyqPqO4nggp84SaFF7yqbFAB25hXwyufrmJaaSWR4GD8e2p5xl7SnYR0b/VwbhZog7lPVV488cWdfvQ+wBGFqB1VnPMLOFQHJYDns23qsTv0E51bUjsOgRS/nceO2EFb1m2P2HixiwvwNTF6wmTJVbhuUxI8v60Cz2Bi/QzM+CjVBhImIqDtowp3K29YBNDVTaQnsXnt8Iti5wrlaAECcW05bJUP/u5xkkNATYhN8DftsHCws4a1vNvG3rzaSX1TCDX1b8bMrOtG6iY1+NqEniDnANBGZgDPh3njgU8+iMqayFOZDdvrxiSA7A0oLnfLwaEjo5kyN3byn00TUrBtE1/c37nNUWFLKe4u28uoX68k9WMSwbgk8OrwznRJs9LM5JtQE8QRwP/AAziytKTgT7BlTfRzIPvGqIHcDRycZrtPYaRYaeJ/zu0UviOsI4TWnY7aktIwZ323j5Xnr2LbvMBe0j+Ox4Z3pm9TY79BMFRTqQLkynNHUr3sbjjEVoKzMWUbzSBI48js/+1idRklOEug52r0y6AUNWlXpDuRzoap8snInf0pZw4acg/RObMjzP+zFhR1s9LM5uVDHQXQE/gB0A472WqlqO4/iMiY0xQXOojjHdR6vhOKDTnlYBDTtAu0vdxLBkZ86jXwNu7KoKl+v280Lc9awYlseHZvVZ8Lt/RnePcFGP5vTCvXa+e/Ar4A/A0Nx5mWyvy5TuQ7tCUgEbjLIWeOMLwCIioXmPaDv7ccSQbOuEFE71yFYunUvz3+6mkUb99CqUR3++KPe3NDXRj+b0IWaIOqo6mfunUxbgGdE5GucpGFMxVKFvExngFlgE1FewAq2sS2cBND5qmNNRI3aVItbSr22eud+/jhnLfNWZRNfP4pnru3GLYOSiI6wJT7NmQk1QRSISBiwzl3jYRtQuQvbmpqptNi5pfS4ZLAcCvLcCgLxHaH1QBhwr3tl0AvqN/U17Kpoa+4hXpy7ho++30796AgeHdaJuy5sS73omtPJbipXqH85jwB1gZ8Cv8FpZhrrUUympio8UG7U8XJnSorSIqc8IgYSujsjjpu7A80SukFUPX/jruJ27XdGP09dnElEuHD/Je0ZP6QdjeraUCVzbk6bINxBcaNV9TEgH6f/ISQiMgJ4GWdFuTdV9bly5bfh3EKL+9oPqOr3ItIZ+GdA1XbA06r6UqjvbXyk6twxtGP58XcS7dl4rE6dJk6z0KDxbjLo6UxeV4NuKfVa3qFiXp+/gckLNlFSqtw8sDU/uawjCQ1s9LOpGKf936iqpSLSP3AkdSjcxPIqcCWQBaSKyCxVzQiotgkY4k7dMRJnWdFBqroG6BPwOtuAmaG+t6lEZaXOiX/H98d3Hh/MOVancRsnAfS+9VjncYOWNfaWUq8dKirh7//dzIT5G8gvLOG63i352ZWdOC/OrrRMxQr169p3wEfuanIHj2xU1Rmn2GcgsF5VNwKIyFTgOuBoglDVBQH1FwGJQV7ncmCD2zluKosqFB+Ggn1weJ/zuyDPeXx4L+Sud0cdr4TiQ84+YZHOWgYdhx27KmjeA2Ia+vc5apDCklKmLs7kL5+vZ3d+IVd0TeDR4Z3o0ryB36GZGirUBNEEyAUuC9imwKkSRCsg4LYTsoBBp6h/D/BJkO03A++fbCcRGQeMA0hKSjrFy9dCqk67f+DJPfCEf9jdfrJtR/oGgolu4CSAfnccSwZNu0CEtXtXtNIyZeZ323hp3lqy9h7m/HZNeGNMf/qfZ6OfjbdCHUkdcr9DgGDtB0GbqERkKE6CuKjc9ihgFPDkKWKbiNM0RXJycshNYNVGWSkU7j/zk/uRx0fGCAQlzrf7Oo0gppHzu0HLY48Dfx9Xr7Hz224p9ZSqMic9mz+lrGHdrnx6tmrI72/oycUd422Qm6kUoY6k/jtBTu6qevcpdssCWgc8TwS2B3ntXjjzOo1U1dxyxSOBpaqaXX6/aqW0OPg3+NOe8POc5BA8rzrCIo4/gddpDE3aBpzcG578hB/dwE7yVdR/1+/m+Tlr+D5zH+2a1uO12/oxskdzSwymUoXaxPSfgMcxwA0EOdmXkwp0FJG2OJ3MNwO3BlYQkSScZqoxqro2yGvcwimalypVccFZNNW424vyT/3a4dHHn8BjWzgjgE91cj/yOKqedfbWIMsy9/HCnNX8d30uLRvG8PwPenFjv1ZEhFsiN5Uv1CamfwU+F5H3gXmn2afEHVQ3B+c210mqmi4i493yCcDTQBzwmvvNqOTIEqYiUhfnDqj7z+gTnSlVWPT6yU/uRx6XFJz6dSLrHX8ib3wexPQud3I/yQk/0m5LrO3WZh/gj3PWkJKRTZN6UTx1TTduG5RETKSNfjb+kTO4c/XYTs44hY9VtUPFh3T2kpOTNS0t7cx3/H0r51t+dEP323mwE3nD49vfA7/NxzS0zllzVjL3HOLP89Yy87tt1I+K4L5L2nH3RW2pb6OfTSURkSVHvpiXF2ofxAGObwjfybEBbtXfz9IhOhbC7NuaqRy7DhTw6ufr+cfirYSJcN/F7XhgSHsa17MvGqbqCLWJqWYvM1VLpn42/ss7XMzErzYw6ZvNFJWWMTq5NQ9f3pHmDa2Z0VQ9oV5B3AB8rqp57vNGwKWq+qF3oRlTcxwuKmXyAmf0c97hYka5o5/bxtvoZ1N1hdrQ+StVPTrVharuE5FfAR96EpUxNcTm3Qf5NH0nb32ziZwDhVzWpRmPDutMt5Y2+tlUfaEmiGD32FkvmjHlqCort+0nJWMnKenZrMk+AMDAtk147bZ+DGjTxOcIjQldqCf5NBF5EWfyPQV+AizxLCpjqpHi0jJSN+1hTvpO5mZksz2vgDCBAW2a8NQ13RjWLYHWTer6HaYxZyzUBPET4CmOTcGdAvzSk4iMqQYOFZXw1docUtKz+Wz1LvIOFxMdEcYlnZrysys7cXnXBJrYHUmmmgv1LqaDwM89jsWYKi03v5DPVu8iJX0nX6/bTWFJGQ3rRHJ512YM69acSzrFUzfKWl5NzRHqXUxzgR+p6j73eWNgqqoO9zA2Y3yXuecQc9J3kpKRTdrmPZQptGpUh1sGJjGsewID2jQh0qbBMDVUqF934o8kBwB3gR9bk9rUOKpKxo79pKRnk5KRzaod+wHo0jyWh4Z2YFj35nRv2cAmzTO1QqgJokxEklR1K4CItOGUU4waU32UlJaRunnv0TuPtu07jAgkn9eYX17dlSu7JdhqbaZWCjVB/B/wjYjMd59fgrtIjzHV0eGiUr5el0NKRjafrcpm76FioiLCuLhDPD+9vAOXd00gvn6032Ea46tQO6k/FZFknKSwDPgIOOxhXMZUuL0Hi/h89S7mpO/kq3U5FBSXERsTweVdmjGse3OGdGpKPZskz5ijQu2kvhd4GGfRn2XA+cBCjl+C1JgqJ2vvIeZmZJOSns3izXsoLVOaN4hhdHJrhnVrzqB21slszMmE+nXpYWAAsEhVh4pIF+BZ78Iy5uyoKmuyD5CSns2c9J2kb3c6mTs2q8/4Ie0Y1q05vRIbWiezMSEINUEUqGqBiCAi0aq62l0TwhjflZYpS7bsJcW9HXXrnkOIQL+kxjw5sgtXdkugXdP6fodpTLUTaoLIcmdw/RCYKyJ7Of2So4jICOBlnBXl3lTV58qV38axdSXygQdU9Xu3rBHOWtU9cO6YultVF4YYr6nhCopL+e/63aSkZzNvVTa5B4uICg/jgg5xjB/Sniu6NaNZrE2hbcy5CLWT+gb34TMi8gXQEPj0VPuISDjO3E1XAllAqojMUtWMgGqbgCHuuIqRwERgkFv2MvCpqv5QRKIAm8ymlss7VMwXa5xO5vlrczhUVEr96AiGdmnG8O4JDOnUlNiYSL/DNKbGOONbNlR1/ulrATAQWK+qGwFEZCpwHXA0QajqgoD6i3A6wRGRBji30t7p1isCis40VlP97cg7fLSTedHGXErKlGax0dzQtxXDujfn/HZNiI6wlQCN8YKX9/S1AjIDnmdx7OogmHuAT9zH7YAc4O8i0htn5tiH3TmhTA2mqqzflU9KhtPJvDwrD4B28fW49+J2DO+eQO/ERoSFWSezMV7zMkEE+x8cdPS1iAzFSRAXuZsigH7AT1T1WxF5GWeywKeC7DsOd9BeUlJSBYRtKltZmfJd5t6j01ts2u18D+jTuhGPj+jMsG7N6dDMOpmNqWxeJogsoHXA80SCdGyLSC+czuiRqpobsG+Wqn7rPp/OSWaTVdWJOH0XJCcn2/Qf1URhSSkLNuSSkp7N3IxsducXEhEmDG4fx90XtWVYtwQSGlgnszF+8jJBpAIdRaQtsA24Gbg1sIKIJAEzgDGquvbIdlXdKSKZItJZVdcAlxPQd2Gqp/0FxXyxehcpGdl8uXoXB4tKqRcVzqVdmjGsWwKXdm5GwzrWyWxMVeFZglDVEhF5CJiDc5vrJFVNF5HxbvkE4GkgDnjNHbhUoqrJ7kv8BHjPvYNpI3CXV7Ea72TvL3A6mTOyWbhhN8WlSnz9KEb1acmwbs0Z3D6OmEjrZDamKhLVmtMqk5ycrGlpaX6HUettyMk/OpJ5WeY+ANrE1WV49+YM655An9aNCbdOZmOqBBFZEvDF/Dg2M5k5Z2VlyvdZ+0jJyCYlfScbcpxO5l6JDXl0WCeGdW9Ox2b1bXoLY6oZSxDmrBSVlLFoYy4pGTuZm5FN9v5CwsOE89s1YewFbbiiawItG9XxO0xjzDmwBGHOSGFJKb//eBUzlm7jQGEJdSLDubRzU4Z1T+Cyzgk0rGudzMbUFJYgTMj2HCzi/nfTSN28lxv7tuKqni24qGO8dTIbU0NZgjAh2ZCTz92TU9mRV8BfbunLtb1b+h2SMcZjliDMaS3YsJvx7y4hMjyM9+87n/7nNfY7JGNMJbAEYU5pWlomv5ixgrbx9Zh05wBaN7FJdY2pLSxBmKDKypQXUtbw+pcbuLhjPH+9tZ+NcjamlrEEYU5QUFzK/0xbxuwVO7llYBK/vq67rdtsTC1kCcIcJ+dAIfe+k8byrH3831VduffitjbAzZhayhKEOWrNzgPcPTmVPQeLmHB7f4Z3b+53SMYYH1mCMADMX5vDQ+8tpU5UONPuH0zPxIZ+h2SM8ZklCMO7i7bwzKx0OiXE8tbYZJsiwxgDWIKo1UrLlN/PXsVb32zisi7NeOWWvtSPtj8JY4zDzga11MHCEh6euox5q7K584I2PHVNN5uC2xhzHEsQtdDOvALueTuVVTv28+yo7oy9oI3fIRljqiBLELXMym153PN2KvkFJbw1dgBDuzTzOyRjTBXl6egnERkhImtEZL2I/DxI+W0istz9WSAivQPKNovIChFZJiK2TFwFmJeRzeg3FhIuwvQHLrDkYIw5Jc+uIEQkHHgVuBLIAlJFZJaqZgRU2wQMUdW9IjISmAgMCigfqqq7vYqxtlBVJv13M7/9OIOerRry5h3JNGsQ43dYxpgqzssmpoHAelXdCCAiU4HrgKMJQlUXBNRfBCR6GE+tVFJaxjP/TmfKoq2M6N6cP9/UhzpRtn6DMeb0vGxiagVkBjzPcredzD3AJwHPFUgRkSUiMu5kO4nIOBFJE5G0nJyccwq4pjlQUMzdb6cxZdFW7h/Sjtdu62fJwRgTMi+vIILdM6lBK4oMxUkQFwVsvlBVt4tIM2CuiKxW1a9OeEHViThNUyQnJwd9/dooa+8h7pmcxoacfP5wY09uGZjkd0jGmGrGywSRBbQOeJ4IbC9fSUR6AW8CI1U198h2Vd3u/t4lIjNxmqxOSBDmRMsy93Hv22kUlpQy+a6BXNQx3u+QjDHVkJdNTKlARxFpKyJRwM3ArMAKIpIEzADGqOragO31RCT2yGNgGLDSw1hrjNkrdnDTGwupExXGzAcvsORgjDlrnl1BqGqJiDwEzAHCgUmqmi4i493yCcDTQBzwmjuldImqJgMJwEx3WwTwD1X91KtYawJV5fX5G3j+0zX0S2rExDuSia8f7XdYxphqTFRrTrN9cnKypqXVviETRSVl/PLDFUxLy+La3i154Ye9iIm0zmhjzOmJyBL3i/kJbCR1NZd3qJjxU5awcGMuP728Iz+7oqMt8GOMqRCWIKqxLbkHuWtyKpl7DvHi6N7c2M+GkRhjKo4liGoqdfMexr2ThgJT7hnEoHZxfodkjKlhLEFUQx8t28ZjHyynVeM6TLpzAG3j6/kdkjGmBrIEUY2oKi9/to6X5q1jUNsmvDGmP43qRvkdljGmhrIEUU0UFJfyxL+W89Gy7fygXyJ/uLEnURGeTsZrjKnlLEFUA7n5hdz/7hLStuzlseGdefDS9nankjHGc5Ygqrj1u/K5e3IqO/cX8Ndb+3JNr5Z+h2SMqSUsQVRhC9bvZvyUJUSGhzF13Pn0S2rsd0jGmFrEEkQVNS0tk1/MWEHb+HpMunMArZvU9TskY0wtYwmiiikrU15IWcPrX27g4o7x/PXWfjSsE+l3WMaYWsgSRBVyuKiU/5m2jE9W7uTWQUk8O6o7keF2p5Ixxh+WIKqIXQcKuO+dJSzP2scvr+7KPRe1tTuVjDG+sgRRBazZeYC7J6ey52ARE27vz/Duzf0OyRhjLEH4bf7aHH783lLqRoUz7f7B9Exs6HdIxhgDWILw1buLtvDMrHQ6JcQy6c5kWjSs43dIxhhzlKc9oCIyQkTWiMh6Efl5kPLbRGS5+7NARHqXKw8Xke9E5D9exlnZSsuUX/87g6c+XMmQTk35YPxgSw7GmCrHsysIEQkHXgWuBLKAVBGZpaoZAdU2AUNUda+IjAQmAoMCyh8GVgENvIqzsh0sLOHhqd8xb9Uu7rqwDb+8uhvhYdYZbYypery8ghgIrFfVjapaBEwFrgusoKoLVHWv+3QRcHTFGxFJBK4G3vQwxkq1I+8wP5qwkM9X7+LX13XnV9d2t+RgjKmyvOyDaAVkBjzP4virg/LuAT4JeP4S8DgQe6o3EZFxwDiApKSks4mzUqzclsc9b6eSX1DCW3cOYGjnZn6HZIwxp+TlFUSwr8YatKLIUJwE8YT7/Bpgl6ouOd2bqOpEVU1W1eSmTZueS7yemZuRzY8mLCRchOkPXGDJwRhTLXh5BZEFtA54nghsL19JRHrhNCONVNVcd/OFwCgRuQqIARqIyBRVvd3DeCucqvLWN5v43exV9GzVkDfvSKZZgxi/wzLGmJB4eQWRCnQUkbYiEgXcDMwKrCAiScAMYIyqrj2yXVWfVNVEVW3j7vd5dUsOJaVl/PLDlfz241UM79acf44bbMnBGFOteHYFoaolIvIQMAcIByaparqIjHfLJwBPA3HAa+60EiWqmuxVTJVlf0ExP35vKV+v2839Q9rxxPAuhFlntDGmmhHVoN0C1VJycrKmpaX5GkPmnkPc83YqG3MO8tvre3DzwKrbcW6MMSKy5GRfzG0kdQX6bute7nsnjcKSMt6+eyAXdoj3OyRjjDlrliAqyOwVO/jZP5fRrEE0U8edT4dmp7w71xhjqjxLEOdIVXntyw28MGcN/c9rzMQx/YmrH+13WMYYc84sQZyDopIy/m/mCj5YksW1vVvywg97ERMZ7ndYxhhTISxBnKV9h4oYP2UJizbu4aeXd+RnV3S0BX6MMTWKJYizsHn3Qe6enErW3sP8+abe3NA38fQ7GWNMNWMJ4gylbt7DuHecW2mn3DuIgW2b+ByRMcZ4wxLEGfjwu208Pn05iY3rMOnOAbSJr+d3SMYY4xlLECFQVV6at46XP1vH+e2aMOH2/jSqG+V3WMYY4ylLEKdRUFzK49OXM+v77fygXyJ/uLEnURGeLsRnjDFVgiWIU8jNL+T+d5eQtmUvjw3vzIOXtrc7lYwxtYYliJNYvyufuyenkr2/gFdv7cfVvVr4HZIxxlQqSxBBLFi/m/FTlhAVEcb7486nX1Jjv0MyxphKZwminGmpmfxi5graxtdj0p0DaN2krt8hGWOMLyxBuMrKlOfnrGHC/A1c3DGeV2/rR4OYSL/DMsYY31iCAA4XlfI/05bxycqd3DooiWdHdScy3O5UMsbUbp6eBUVkhIisEZH1IvLzIOW3ichy92eBiPR2t8eIyGIR+V5E0kXkWa9izDtUzM0TF/Jp+k5+eXVXfnd9D0sOxhiDh1cQIhIOvApcCWQBqSIyS1UzAqptAoao6l4RGQlMBAYBhcBlqpovIpHANyLyiaouqug468dE0Ca+Hj8e2oFh3ZtX9MsbY0y15WUT00BgvapuBBCRqcB1wNEEoaoLAuovAhLd7Qrku9sj3R9P1kYNDxNevrmvFy9tjDHVmpdtKa2AzIDnWe62k7kH+OTIExEJF5FlwC5grqp+G2wnERknImkikpaTk3PuURtjjAG8TRDBhhwHvQoQkaE4CeKJoxVVS1W1D85VxUAR6RFsX1WdqKrJqprctGnTc4/aGGMM4G2CyAJaBzxPBLaXryQivYA3getUNbd8uaruA74ERngSpTHGmKC8TBCpQEcRaSsiUcDNwKzACiKSBMwAxqjq2oDtTUWkkfu4DnAFsNrDWI0xxpTjWSe1qpaIyEPAHCAcmKSq6SIy3i2fADwNxAGvuZPglahqMtACeNu9EyoMmKaq//EqVmOMMScS54ahmiE5OVnT0tL8DsMYY6oNEVnifjE/gY0IM8YYE5QlCGOMMUHVqCYmEckBtpzl7vHA7goMp6JYXGfG4jozFteZqYlxnaeqQccI1KgEcS5EJO1k7XB+srjOjMV1ZiyuM1Pb4rImJmOMMUFZgjDGGBOUJYhjJvodwElYXGfG4jozFteZqVVxWR+EMcaYoOwKwhhjTFCWIIwxxgRVqxJECEugioi84pYvF5F+VSSuS0UkT0SWuT9PV1Jck0Rkl4isPEm5X8frdHH5dbxai8gXIrLKXSr34SB1Kv2YhRhXpR+zUJYW9ul4hRKXL39j7nuHi8h3InLC/HQVfrxUtVb84EwYuAFoB0QB3wPdytW5CmfRIgHOB76tInFdCvzHh2N2CdAPWHmS8ko/XiHG5dfxagH0cx/HAmuryN9YKHFV+jFzj0F993Ek8C1wfhU4XqHE5cvfmPve/wP8I9j7V/Txqk1XEEeXQFXVIuDIEqiBrgPeUccioJGItKgCcflCVb8C9pyiih/HK5S4fKGqO1R1qfv4ALCKE1dRrPRjFmJclc49BqdbWtiP4xVKXL4QkUTgapw1dIKp0ONVmxJEKEugnukyqZUVF8Bg95L3ExHp7nFMofLjeIXK1+MlIm2AvjjfPgP5esxOERf4cMzk9EsL+3K8QogL/Pkbewl4HCg7SXmFHq/alCBCWQI15GVSK1Ao77kUZ76U3sBfgA89jilUfhyvUPh6vESkPvAv4BFV3V++OMgulXLMThOXL8dMT7+0sC/HK4S4Kv14icg1wC5VXXKqakG2nfXxqk0JIpQlUENaJrWy41LV/UcueVV1NhApIvEexxUKP47Xafl5vEQkEuck/J6qzghSxZdjdrq4/P4b05MvLezr39jJ4vLpeF0IjBKRzThN0ZeJyJRydSr0eNWmBHHaJVDd53e4dwKcD+Sp6g6/4xKR5iLOknsiMhDn3+2E9bt94MfxOi2/jpf7nm8Bq1T1xZNUq/RjFkpcfhwzCW1pYT+O12nj8uN4qeqTqpqoqm1wzhOfq+rt5apV6PHybMnRqkZDWwJ1Ns5dAOuBQ8BdVSSuHwIPiEgJcBi4Wd1bFrwkIu/j3K0RLyJZwK9wOux8O14hxuXL8cL5hjcGWOG2XwP8AkgKiM2PYxZKXH4cs6BLC/v9fzLEuPz6GzuBl8fLptowxhgTVG1qYjLGGHMGLEEYY4wJyhKEMcaYoCxBGGOMCcoShDHGmKAsQRhTBYgzO+gJs3Ma4ydLEMYYY4KyBGHMGRCR28VZK2CZiLzhTuqWLyJ/EpGlIvKZiDR16/YRkUXizMs/U0Qau9s7iMg8d6K3pSLS3n35+iIyXURWi8h7R0bqGuMXSxDGhEhEugI3ARe6E7mVArcB9YClqtoPmI8zshvgHeAJVe0FrAjY/h7wqjvR2wXAkakQ+gKPAN1w1ge50OOPZMwp1ZqpNoypAJcD/YFU98t9HZzpoMuAf7p1pgAzRKQh0EhV57vb3wY+EJFYoJWqzgRQ1QIA9/UWq2qW+3wZ0Ab4xvNPZcxJWIIwJnQCvK2qTx63UeSpcvVONX/NqZqNCgMel2L/P43PrInJmNB9BvxQRJoBiEgTETkP5//RD906twLfqGoesFdELna3jwHmu+swZInI9e5rRItI3cr8EMaEyr6hGBMiVc0QkV8CKSISBhQDPwYOAt1FZAmQh9NPATAWmOAmgI0cm1lzDPCGiPzafY0fVeLHMCZkNpurMedIRPJVtb7fcRhT0ayJyRhjTFB2BWGMMSYou4IwxhgTlCUIY4wxQVmCMMYYE5QlCGOMMUFZgjDGGBPU/wfJmsMacfGzYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_history.history.keys())\n",
    "history = train_history.history\n",
    "# summarize history for accuracy\n",
    "plt.plot(history['sparse_categorical_accuracy'])\n",
    "plt.plot(history['val_sparse_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking accuracy on train \n",
    "# predictions = model.predict(train_input)\n",
    "# accuracy_score (y_train,  np.argmax(predictions, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding test data into BERT Format. If you have  preprocessing/cleaning of input text, it should be done before this step\n",
    "test_input = encode_text_for_bert2((df_test['annotation'] + ' Student Note: ' + df_test['student_note']).values, tokenizer_for_bert, max_len= max_len)\n",
    "y_test = le.transform(df_test['phrases'].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 100s 318ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29111287442967665"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking accuracy on test data. You may be able to improve it by taking bigger length of text, more epochs or by adding more dense layers into the model\n",
    "predictions = model.predict(test_input)\n",
    "accuracy_score (y_test,  np.argmax(predictions, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting top 3 predictions\n",
    "top3 = predictions.argsort(axis = 1)[:, -3:]\n",
    "pred_top = le.inverse_transform(top3[:, -1])\n",
    "pred_top_2nd = le.inverse_transform(top3[:, -2])\n",
    "pred_top_3rd = le.inverse_transform(top3[:, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding top 3 predictions to dataframe\n",
    "df_test['inferred_phrase_top'] = pred_top.tolist()\n",
    "df_test['inferred_phrase_top_2nd'] = pred_top_2nd.tolist()\n",
    "df_test['inferred_phrase_top_3rd'] = pred_top_3rd.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving fine-tuned(trained) model to disk\n",
    "model.save('./bertmodel_feedback_prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model for inference\n",
    "model = load_model('./bertmodel_feedback_prediction')\n",
    "\n",
    "#Loading Label encoder\n",
    "file = open('le.obj', 'rb')\n",
    "le = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "Annotaion: banks had very few rules about loaning out money. \n",
      "Student Note: Seems like the banks should have done something sooner\n",
      "\n",
      "Inferred Feedback Phrases:  1)Good job!  2)Why?  3)Explain why this is important.\n"
     ]
    }
   ],
   "source": [
    "annotation = \"banks had very few rules about loaning out money.\" \n",
    "student_note = \"Seems like the banks should have done something sooner\"\n",
    "prediction = model.predict (  encode_text_for_bert2 ( [annotation + ' Student Note: ' + student_note] , tokenizer_for_bert, max_len=max_len) ) \n",
    "top3 = prediction.argsort(axis = 1)[:, -3:]\n",
    "pred_top = le.inverse_transform(top3[:, -1])\n",
    "pred_top_2nd = le.inverse_transform(top3[:, -2])\n",
    "pred_top_3rd = le.inverse_transform(top3[:, -3])\n",
    "\n",
    "print ('Annotaion:', annotation , '\\nStudent Note:', student_note)\n",
    "print(f'\\nInferred Feedback Phrases:  1){pred_top[0]}  2){pred_top_2nd[0]}  3){pred_top_3rd[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "Annotaion: night was dark. The wind blew. \n",
      "Student Note: there are 13 stanzas \n",
      "\n",
      "Inferred Feedback Phrases:  1)Good job!  2)Why?  3)Add more detail.\n"
     ]
    }
   ],
   "source": [
    "annotation = \"night was dark. The wind blew.\" \n",
    "student_note = \"there are 13 stanzas \"\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert2 ( [annotation + ' Student Note: ' + student_note] , tokenizer_for_bert, max_len=max_len) ) \n",
    "top3 = prediction.argsort(axis = 1)[:, -3:]\n",
    "pred_top = le.inverse_transform(top3[:, -1])\n",
    "pred_top_2nd = le.inverse_transform(top3[:, -2])\n",
    "pred_top_3rd = le.inverse_transform(top3[:, -3])\n",
    "\n",
    "print ('Annotaion:', annotation , '\\nStudent Note:', student_note)\n",
    "print(f'\\nInferred Feedback Phrases:  1){pred_top[0]}  2){pred_top_2nd[0]}  3){pred_top_3rd[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "Annotaion: He was heart-broken. \n",
      "Student Note: She is so caught up in trying to fit in she never realizes how she is hurting her husband.\n",
      "\n",
      "Inferred Feedback Phrases:  1)Add more detail.  2)Explain why this is important.  3)Why?\n"
     ]
    }
   ],
   "source": [
    "annotation = \"He was heart-broken.\" \n",
    "student_note = \"She is so caught up in trying to fit in she never realizes how she is hurting her husband.\"\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert2 ( [annotation + ' Student Note: ' + student_note] , tokenizer_for_bert, max_len=max_len) ) \n",
    "top3 = prediction.argsort(axis = 1)[:, -3:]\n",
    "pred_top = le.inverse_transform(top3[:, -1])\n",
    "pred_top_2nd = le.inverse_transform(top3[:, -2])\n",
    "pred_top_3rd = le.inverse_transform(top3[:, -3])\n",
    "\n",
    "print ('Annotaion:', annotation , '\\nStudent Note:', student_note)\n",
    "#print('\\nInferred Feedback Phrase: ', le.inverse_transform( np.argmax(prediction).reshape(-1, 1) )[0] )\n",
    "print(f'\\nInferred Feedback Phrases:  1){pred_top[0]}  2){pred_top_2nd[0]}  3){pred_top_3rd[0]}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
